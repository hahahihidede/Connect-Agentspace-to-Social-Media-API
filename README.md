# Agentspace Demo: Social Media Trend Analysis with Google Cloud Platform

This project demonstrates the construction of an "Agentspace" — an intelligent conversational AI interface — designed to analyze and answer questions about trends across various social media platforms. The trend data is automatically collected and stored in Google BigQuery. The primary goal of this demo is to showcase how Google Cloud Platform (GCP) can integrate automated data pipelines with generative AI capabilities to provide real-time and interactive business insights.

---

## 🚀 Architecture

### 2.1. GCP Service Architecture

Here's an explanation of the workflow and the role of each GCP service in this architecture:

* **Cloud Scheduler**: Automatically triggers the `trend-generator` Cloud Run service every 2 minutes.
* **Cloud Run (`trend-generator`)**: Generates rich dummy trend data (from platforms like Facebook, TikTok, Twitter, Instagram) and ingests this data into the `daily_trends` and `social_media_posts` tables in BigQuery.
* **BigQuery**: Serves as a centralized, structured data repository for all ingested trend data.
* **Vertex AI Search (Data Store)**: Data from BigQuery is synchronized to a Data Store in Vertex AI Search. This enables the AI agent to perform *Retrieval Augmented Generation* (RAG) from your structured data.
* **Vertex AI Agent Builder**: The AI agent (`SocialMediaTrendAnalyzer`) built on this platform directly interacts with users. When users ask trend-related questions, this agent leverages the Vertex AI Search Data Store to find and retrieve relevant information from BigQuery, then uses a Large Language Model (LLM) to compose informative answers.

### 2.2. Data Flow Diagram

This diagram illustrates how Google Cloud Platform can automate social media trend analysis using a conversational AI agent. Every two minutes, Cloud Scheduler triggers a Cloud Function to fetch the latest data from various social media APIs like Facebook, Instagram, TikTok, and Twitter. The collected data is stored in BigQuery as a central repository, then synchronized to Vertex AI Search (Data Store). The AI agent named `SocialMediaTrendAnalyzer`, built in Vertex AI Agent Builder, then uses this data to answer user questions intelligently and in real-time through Agentspace, creating an interactive and responsive analytical experience.

---

## 🛠️ GCP Services Used

Here are more details about the GCP services utilized in this project:

### 3.1. BigQuery
* **Role**: A fully managed data warehouse for scalable and cost-effective storage of social media trend data. It acts as the "single source of truth" for data analyzed by the agent.
* **Details**: The `social_media_trends` dataset contains two main tables: `daily_trends` (trend summaries) and `social_media_posts` (individual post details).

### 3.2. Cloud Run
* **Role**: An event-driven, serverless compute service. Used as the trend data generator. It scales from zero (no cost when idle) to meet demand.
* **Service Name**: `trend-generator`
* **Language**: Python 3.11
* **Base Image**: `python:3.11-slim-buster`

### 3.3. Cloud Scheduler
* **Role**: A managed cron job scheduler that periodically triggers the Cloud Run service (every 2 minutes).
* **Job Name**: `trigger-trend-generator`

### 3.4. Vertex AI Search (Discovery Engine - Data Store)
* **Role**: Allows you to create a powerful search engine from your data. It is used to index BigQuery data and make it available for information retrieval by the AI agent.
* **Type**: Structured Data Store directly connected to BigQuery tables.

### 3.5. Vertex AI Agent Builder
* **Role**: A development platform for building AI conversational agents powered by Large Language Models (LLM). It orchestrates user interactions, tool calls, and LLM responses.
* **Agent Name**: `SocialMediaTrendAnalyzer`
* **Region**: `us-central1` (Primary region for GenAI services)
* **Model**: Gemini (or Gemini Pro)

### 3.6. Cloud Function (`query-bigquery-trends`) - (Optional/Alternative Tool Connection)
* **Role**: A serverless function that can act as an API gateway for querying BigQuery if you choose a "Custom Tool" approach in Agent Builder, rather than a Data Store. This function is deployed as a private function requiring OIDC authentication.
* **Function Name**: `query-bigquery-trends`
* **URL (Example)**: `https://asia-southeast2-YOUR_GCP_PROJECT_ID.cloudfunctions.net/query-bigquery-trends` (This URL is needed if you choose to use Cloud Function as a Custom Tool).

---

## 🚀 Deployment Steps

### A. Social Media API & Environment Preparation (Conceptual)

*(Note: This demo uses dummy data generated by Cloud Run, so actual API preparation is not required. This section is conceptual to illustrate what would be needed in a real-world scenario.)*

* **Understand Platform Policies**: Each platform (Meta/Facebook/Instagram, TikTok, X/Twitter) has distinct API policies, terms of service, and data usage limitations. It's crucial to thoroughly understand these.
* **Register as a Developer & Create an App**: Visit the developer portal for each platform (e.g., Meta for Developers, TikTok for Developers, X Developer Platform). Create a developer account and register a new application (e.g., "Social Media Trend Analyzer App") for your project. This app will be your identity when calling APIs.
* **Request API Access & Undergo Review**: Many social media APIs, especially for large-scale or user-related data, require strict approval and review processes, often involving detailed explanations of your use case. Obtain credentials such as API Key, API Secret, Client ID, and Client Secret.
* **Understand Authentication & Tokens**: Learn each API's authentication mechanism (commonly OAuth 2.0) to acquire the necessary Access Token for calling API endpoints.
* **Observe Rate Limits**: Each API has limits on the number of requests allowed within a specific period (e.g., per second, per minute, per hour). Your data ingestion pipeline must be designed to adhere to these limits.
* **Use Official SDK/Library**: Utilize official SDKs or libraries provided by the platforms (if available) to simplify API interaction.

### B. GCP Environment Preparation

1.  **Choose Your GCP Project**: Replace `YOUR_GCP_PROJECT_ID` with your actual project ID. Ensure billing is activated.
2.  **Open Cloud Shell**: Access the terminal console in your browser.
3.  **Configure `gcloud` CLI**:
    ```bash
    gcloud config set project YOUR_GCP_PROJECT_ID
    gcloud components install alpha # For specific commands
    gcloud auth application-default login
    ```

### C. BigQuery Setup (Trend Data Hub)

1.  **Enable BigQuery API**: Navigate to `APIs & Services > Enabled APIs & Services`.
2.  **Create BigQuery Dataset**:
    * In the BigQuery Console, create a dataset with ID: `social_media_trends`.
    * Data location: `asia-southeast2` (Kuala Lumpur, current time is Wednesday, July 30, 2025 at 5:18:03 PM +08).
3.  **Create BigQuery Table `daily_trends`**:
    * Table name: `daily_trends`.
    * Schema (text mode):
        ```json
        [
            {"name": "keyword", "type": "STRING", "mode": "NULLABLE"},
            {"name": "date", "type": "DATE", "mode": "NULLABLE"},
            {"name": "interest_score", "type": "INTEGER", "mode": "NULLABLE"},
            {"name": "timestamp", "type": "TIMESTAMP", "mode": "NULLABLE"},
            {"name": "platform", "type": "STRING", "mode": "NULLABLE"},
            {"name": "source_api", "type": "STRING", "mode": "NULLABLE"},
            {"name": "item_id", "type": "STRING", "mode": "NULLABLE"},
            {"name": "content_text", "type": "STRING", "mode": "NULLABLE"},
            {"name": "views_impressions_plays", "type": "INTEGER", "mode": "NULLABLE"},
            {"name": "likes_count", "type": "INTEGER", "mode": "NULLABLE"},
            {"name": "comments_count", "type": "INTEGER", "mode": "NULLABLE"},
            {"name": "shares_count", "type": "INTEGER", "mode": "NULLABLE"},
            {"name": "hashtags_list", "type": "STRING", "mode": "NULLABLE"},
            {"name": "url_link", "type": "STRING", "mode": "NULLABLE"},
            {"name": "demographic_summary", "type": "STRING", "mode": "NULLABLE"},
            {"name": "region_summary", "type": "STRING", "mode": "NULLABLE"},
            {"name": "total_posts_count", "type": "INTEGER", "mode": "NULLABLE"}
        ]
        ```
    * Click "CREATE TABLE".
4.  **Create BigQuery Table `social_media_posts`**:
    * Table name: `social_media_posts`.
    * Schema (text mode):
        ```json
        [
            {"name": "post_id", "type": "STRING", "mode": "REQUIRED"},
            {"name": "trend_keyword", "type": "STRING", "mode": "NULLABLE"},
            {"name": "platform", "type": "STRING", "mode": "NULLABLE"},
            {"name": "post_timestamp", "type": "TIMESTAMP", "mode": "NULLABLE"},
            {"name": "post_type", "type": "STRING", "mode": "NULLABLE", "description": "e.g., Image, Video, Text"},
            {"name": "post_text", "type": "STRING", "mode": "NULLABLE"},
            {"name": "post_url", "type": "STRING", "mode": "NULLABLE"},
            {"name": "post_likes_count", "type": "INTEGER", "mode": "NULLABLE"},
            {"name": "post_comments_count", "type": "INTEGER", "mode": "NULLABLE"},
            {"name": "post_shares_count", "type": "INTEGER", "mode": "NULLABLE"},
            {"name": "post_hashtags", "type": "STRING", "mode": "NULLABLE"},
            {"name": "username", "type": "STRING", "mode": "NULLABLE"},
            {"name": "user_id", "type": "STRING", "mode": "NULLABLE"}
        ]
        ```
    * Click "CREATE TABLE".

### D. Cloud Run (Data Generator: `trend-generator`)

1.  **Enable Cloud Run & Artifact Registry APIs**.
2.  **Create Working Directory & Files**:
    ```bash
    mkdir ~/trend_generator
    cd ~/trend_generator
    ```
3.  **Create `Dockerfile`**:
    ```dockerfile
    FROM python:3.11-slim-buster

    WORKDIR /app

    COPY requirements.txt .

    RUN pip install --no-cache-dir -r requirements.txt

    COPY main.py .

    CMD ["functions-framework", "--target", "generate_and_store_trends", "--port", "8080"]
    ```
4.  **Create `requirements.txt`**:
    ```
    google-cloud-bigquery
    functions-framework
    ```
5.  **Create `main.py`**:
    ```python
    import os
    import datetime
    import random
    from google.cloud import bigquery
    import functions_framework
    import json

    print("DEBUG: Script started. (Global scope - trend_generator)")

    PROJECT_ID = os.environ.get("GCP_PROJECT_ID", "YOUR_GCP_PROJECT_ID")
    DATASET_ID = "social_media_trends"
    DAILY_TRENDS_TABLE = "daily_trends"
    POSTS_TABLE = "social_media_posts"
    print(f"DEBUG: Project/Dataset/Table IDs set. (Global scope - trend_generator)")


    @functions_framework.http
    def generate_and_store_trends(request):
        print("DEBUG: generate_and_store_trends function called.")
        try:
            client = bigquery.Client()
            print("DEBUG: BigQuery client initialized inside function.")

            rows_for_daily_trends = []
            rows_for_posts = []

            current_timestamp = datetime.datetime.utcnow().replace(microsecond=0).isoformat() + "Z"
            today = datetime.date.today()

            platforms_data = {
                'Facebook': {'source': 'Meta Graph API (Ads Archive)', 'topics': ['Promo Gadget', 'Fashion Sale', 'Local Events', 'Health Tips'], 'id_prefix': 'FB_AD_', 'url_base': '[https://facebook.com/ads/](https://facebook.com/ads/)', 'post_types': ['Image', 'Video']},
                'TikTok': {'source': 'TikTok Search API', 'topics': ['Dance Challenge', 'Cooking Hacks', 'Travel Vlog', 'DIY Crafts'], 'id_prefix': 'TIK_VIDEO_', 'url_base': '[https://tiktok.com/@user/video/](https://tiktok.com/@user/video/)', 'post_types': ['Video']},
                'Twitter': {'source': 'X API (Trends/Place)', 'topics': ['Breaking News', 'Political Debate', 'Sports Update', 'Tech Insights'], 'id_prefix': 'TW_TWEET_', 'url_base': '[https://twitter.com/status/](https://twitter.com/status/)', 'post_types': ['Text', 'Image', 'Video']},
                'Instagram': {'source': 'Instagram Media API', 'topics': ['Photography Tips', 'Fitness Journey', 'Art Showcase', 'Food Photography'], 'id_prefix': 'IG_POST_', 'url_base': '[https://instagram.com/p/](https://instagram.com/p/)', 'post_types': ['Image', 'Video']}
            }

            for platform, p_data in platforms_data.items():
                for topic in p_data['topics']:
                    total_views_impressions_plays = random.randint(5_000_000, 20_000_000)
                    total_likes_count = total_views_impressions_plays // random.randint(10, 20)
                    total_comments_count = total_likes_count // random.randint(20, 50)
                    total_shares_count = total_likes_count // random.randint(10, 30)

                    keyword = f"{topic} trends on {platform}"
                    content_text_summary = f"Overall trends for {topic.lower()} on {platform} today."
                    if platform == 'Twitter':
                        keyword = f"#{topic.lower().replace(' ', '')} trending"
                    elif platform == 'TikTok':
                        keyword = f"Viral {topic.lower()} Challenge"
                    elif platform == 'Instagram':
                        keyword = f"{topic} Inspiration"
                    else: # Facebook
                        keyword = f"{topic} Discussion on Facebook"


                    demog_data = [
                        {"age": "18-24", "gender": "male", "percentage": round(random.uniform(0.1, 0.4), 2)},
                        {"age": "25-34", "gender": "female", "percentage": round(random.uniform(0.1, 0.3), 2)},
                        {"age": "35-44", "gender": "other", "percentage": round(random.uniform(0.05, 0.2), 2)}
                    ]
                    demog_summary = json.dumps(demog_data)

                    region_data = [
                        {"region": "Jakarta", "percentage": round(random.uniform(0.3, 0.6), 2)},
                        {"region": "Surabaya", "percentage": round(random.uniform(0.1, 0.3), 2)},
                        {"region": "Bandung", "percentage": round(random.uniform(0.05, 0.15), 2)}
                    ]
                    region_summary = json.dumps(region_data)

                    base_hashtags = [f"#{t.lower().replace(' ', '')}" for t in topic.split()]
                    platform_hashtags = [f"#{platform.lower()}trends", f"#{platform.lower()}{topic.replace(' ', '')}"]
                    all_hashtags = list(set(base_hashtags + platform_hashtags))
                    hashtags_list_str = " ".join(all_hashtags)

                    trend_id = f"TREND_{platform}_{topic.replace(' ', '_')}_{today.strftime('%Y%m%d')}"
                    url_link_summary = f"[https://trendsummaries.com/](https://trendsummaries.com/){trend_id}"

                    total_posts_for_trend = random.randint(50, 500)

                    rows_for_daily_trends.append({
                        "keyword": keyword,
                        "date": today.isoformat(),
                        "interest_score": total_likes_count + total_comments_count + total_shares_count,
                        "timestamp": current_timestamp,
                        "platform": platform,
                        "source_api": p_data['source'],
                        "item_id": trend_id,
                        "content_text": content_text_summary,
                        "views_impressions_plays": total_views_impressions_plays,
                        "likes_count": total_likes_count,
                        "comments_count": total_comments_count,
                        "shares_count": total_shares_count,
                        "hashtags_list": hashtags_list_str,
                        "url_link": url_link_summary,
                        "demographic_summary": demog_summary,
                        "region_summary": region_summary,
                        "total_posts_count": total_posts_for_trend
                    })

                    num_posts_to_generate = random.randint(3, 8)
                    for i in range(num_posts_to_generate):
                        post_likes = random.randint(100, 5000)
                        post_comments = post_likes // random.randint(10, 20)
                        post_shares = post_likes // random.randint(5, 10)
                        post_type = random.choice(p_data['post_types'])
                        username = f"user_{random.randint(1000,9999)}"
                        user_id = f"UID_{random.randint(10000,99999)}"

                        post_id = f"POST_{platform}_{topic.replace(' ', '_')}_{random.randint(100000, 999999)}"
                        post_url = f"{p_data['url_base']}{post_id}"
                        post_text = f"Amazing content on {topic.lower()}! #{topic.replace(' ', '')} #{platform} #{post_type.lower()}"

                        rows_for_posts.append({
                            "post_id": post_id,
                            "trend_keyword": keyword,
                            "platform": platform,
                            "post_timestamp": current_timestamp,
                            "post_type": post_type,
                            "post_text": post_text,
                            "post_url": post_url,
                            "post_likes_count": post_likes,
                            "post_comments_count": post_comments,
                            "post_shares_count": post_shares,
                            "post_hashtags": hashtags_list_str,
                            "username": username,
                            "user_id": user_id
                        })

            print(f"DEBUG: Generated total {len(rows_for_daily_trends)} trend summaries and {len(rows_for_posts)} individual posts.")

            try:
                daily_trends_table_ref = client.dataset(DATASET_ID).table(DAILY_TRENDS_TABLE)
                print(f"DEBUG: Inserting {len(rows_for_daily_trends)} rows to {DAILY_TRENDS_TABLE}")
                errors_daily_trends = client.insert_rows_json(daily_trends_table_ref, rows_for_daily_trends)
                if errors_daily_trends == []:
                    print(f"DEBUG: Successfully inserted {len(rows_for_daily_trends)} rows into {DAILY_TRENDS_TABLE}.")
                else:
                    print(f"DEBUG ERROR: Errors inserting into {DAILY_TRENDS_TABLE}: {errors_daily_trends}")

                posts_table_ref = client.dataset(DATASET_ID).table(POSTS_TABLE)
                print(f"DEBUG: Inserting {len(rows_for_posts)} rows to {POSTS_TABLE}")
                errors_posts = client.insert_rows_json(posts_table_ref, rows_for_posts)
                if errors_posts == []:
                    print(f"DEBUG: Successfully inserted {len(rows_for_posts)} rows into {POSTS_TABLE}.")
                else:
                    print(f"DEBUG ERROR: Errors inserting into {POSTS_TABLE}: {errors_posts}")

                if errors_daily_trends == [] and errors_posts == []:
                    return f"Successfully inserted {len(rows_for_daily_trends)} trends and {len(rows_for_posts)} posts.", 200
                else:
                    return f"Errors encountered during inserts. Check logs.", 500

            except Exception as e:
                print(f"DEBUG CRITICAL ERROR: An error occurred during BigQuery insert: {e}")
                return f"An error occurred during BigQuery insert: {e}", 500

        except Exception as e:
            print(f"DEBUG CRITICAL ERROR: An error occurred inside function: {e}")
            return f"An error occurred: {e}", 500
    ```
6.  **Build & Deploy Cloud Run Service**:
    Replace `YOUR_GCP_PROJECT_ID` with your actual GCP project ID. Note the Cloud Run Service URL from the output of the `gcloud run deploy` command.
    ```bash
    docker build -t asia-southeast2-docker.pkg.dev/YOUR_GCP_PROJECT_ID/cloud-run-source-deploy/trend-generator:latest .
    docker push asia-southeast2-docker.pkg.dev/YOUR_GCP_PROJECT_ID/cloud-run-source-deploy/trend-generator:latest
    gcloud run deploy trend-generator \
    --image asia-southeast2-docker.pkg.dev/YOUR_GCP_PROJECT_ID/cloud-run-source-deploy/trend-generator:latest \
    --region asia-southeast2 \
    --allow-unauthenticated \
    --set-env-vars GCP_PROJECT_ID=YOUR_GCP_PROJECT_ID \
    --min-instances 1 \
    --max-instances 1 \
    --platform managed \
    --no-cpu-throttling \
    --memory 512Mi \
    --port 8080
    ```

### E. Cloud Function (BigQuery Query Endpoint - Optional)

*(This section is only relevant if you choose to use Cloud Function as a Custom Tool, not Data Store.)*

1.  **Enable Cloud Functions API**.
2.  **Verify Content of Function Directory (`~/bigquery_query_function/`)**:
    * `requirements.txt`:
        ```
        google-cloud-bigquery
        functions-framework
        ```
    * `main.py`:
        ```python
        import os
        from google.cloud import bigquery
        import functions_framework
        import json
        from typing import List, Dict, Any, Optional

        client = bigquery.Client()
        PROJECT_ID = os.environ.get("GCP_PROJECT_ID", "YOUR_GCP_PROJECT_ID")
        DATASET_ID = "social_media_trends"
        TABLE_ID = "daily_trends" # This function currently only queries daily_trends or would need adjustment for social_media_posts

        @functions_framework.http
        def query_trends(request):
            headers = {
                'Access-Control-Allow-Origin': '*',
                'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
                'Access-Control-Allow-Headers': 'Content-Type',
                'Access-Control-Max-Age': '3600'
            }

            if request.method == 'OPTIONS':
                return ('', 204, headers)

            try:
                request_json = request.get_json(silent=True)
                if not request_json:
                    return ("Please provide a JSON body with 'keyword'.", 400, headers)

                keyword = request_json.get("keyword")
                platform = request_json.get("platform")
                limit = request_json.get("limit", 5)

                if not keyword:
                    return ("Missing 'keyword' in request body.", 400, headers)

                # Query for daily_trends
                select_columns_daily = """
                keyword, date, interest_score, platform, source_api, item_id,
                content_text, views_impressions_plays, likes_count, comments_count,
                shares_count, hashtags_list, url_link, demographic_summary, region_summary, total_posts_count
                """
                where_clause_daily = f"WHERE LOWER(keyword) LIKE LOWER('%{keyword}%')"
                if platform:
                    where_clause_daily += f" AND LOWER(platform) = LOWER('{platform}')"
                query_daily = f"""
                SELECT {select_columns_daily}
                FROM `{PROJECT_ID}.{DATASET_ID}.daily_trends`
                {where_clause_daily}
                ORDER BY timestamp DESC
                LIMIT {limit}
                """

                # Query for social_media_posts (if needed)
                # You can add logic here to query the social_media_posts table
                # based on item_id from daily_trends or other parameters

                query_job = client.query(query_daily) # Run query to daily_trends
                results = [dict(row) for row in query_job.result()]

                if not results:
                    return (json.dumps({"message": f"No trends found for keyword: {keyword} on platform: {platform or 'any'}"}), 200, {'Content-Type': 'application/json', **headers})

                return (json.dumps(results), 200, {'Content-Type': 'application/json', **headers})

            except Exception as e:
                print(f"Error querying BigQuery: {e}")
                return (f"Error processing request: {e}", 500, headers)
        ```
3.  **Deploy Cloud Function**:
    ```bash
    gcloud functions deploy query-bigquery-trends \
    --runtime python311 \
    --trigger-http \
    --entry-point query_trends \
    --set-env-vars GCP_PROJECT_ID=YOUR_GCP_PROJECT_ID \
    --region asia-southeast2 \
    --project=YOUR_GCP_PROJECT_ID \
    --memory 256MB \
    --timeout 300s \
    --min-instances 1
    ```
    If prompted "Allow unauthenticated invocations...?", answer `N`. Note the Cloud Function Trigger URL from the output.

### F. Cloud Scheduler (Data Generator Trigger: `trigger-trend-generator`)

1.  **Enable Cloud Scheduler API**.
2.  **Create Cloud Scheduler Job**:
    Replace `YOUR_PROJECT_NUMBER` with your GCP Project Number (use `gcloud projects describe YOUR_GCP_PROJECT_ID --format="value(projectNumber)"`). Replace `YOUR_COMPUTE_SERVICE_ACCOUNT_EMAIL` with your Compute Engine Default Service Account email (e.g., `[PROJECT_NUMBER]-compute@developer.gserviceaccount.com`). Replace `YOUR_CLOUD_RUN_URL` with the URL of your `trend-generator` Cloud Run Service.
    ```bash
    gcloud scheduler jobs create http trigger-trend-generator \
    --location=asia-southeast2 \
    --schedule="*/2 * * * *" \
    --uri="YOUR_CLOUD_RUN_URL" \
    --http-method=POST \
    --message-body="json={}" \
    --oidc-service-account-email="YOUR_COMPUTE_SERVICE_ACCOUNT_EMAIL" \
    --oidc-token-audience="YOUR_CLOUD_RUN_URL" \
    --headers="Content-Type=application/json" \
    --description="Trigger Cloud Run trend generator every 2 minutes"
    ```
3.  **Verify IAM for Cloud Scheduler**:
    Ensure the Compute Engine Default Service Account (`YOUR_COMPUTE_SERVICE_ACCOUNT_EMAIL`) has the `roles/run.invoker` role on the `trend-generator` Cloud Run service.
    ```bash
    gcloud run services add-iam-policy-binding trend-generator \
    --member=serviceAccount:YOUR_COMPUTE_SERVICE_ACCOUNT_EMAIL \
    --role=roles/run.invoker \
    --region asia-southeast2
    ```

### G. Verify Data Flow to BigQuery

1.  **Force Execute Cloud Scheduler**: In the Cloud Scheduler console, select the `trigger-trend-generator` job and click "FORCE RUN". Verify its status (expected: SUCCESS or 200 OK).
2.  **Check Data in BigQuery (Both Tables)**:
    * `daily_trends`:
        ```sql
        SELECT * FROM `YOUR_GCP_PROJECT_ID.social_media_trends.daily_trends` ORDER BY timestamp DESC LIMIT 20
        ```

### H. Vertex AI Search (Data Store - Agentspace Connection to BigQuery)

1.  **Enable Vertex AI Search API (Discovery Engine)**.
2.  **Create Data Store**:
    * In the GCP console, navigate to **Vertex AI > Agent Builder > Discovery Engines > Data Stores**.
    * Click "CREATE NEW DATA STORE".
    * Choose application type: Select `Search for customer-facing applications`.
    * Connect data source: Select `Data in BigQuery`.
    * Select your BigQuery dataset and table: `social_media_trends.daily_trends`.
    * Follow the steps to name your Data Store.

### I. Vertex AI Agent Builder (Building & Testing Agentspace)

1.  **Enable Agent Builder API**.
2.  **Create "SocialMediaTrendAnalyzer" Agent**:
    * In the GCP console, navigate to **Vertex AI > Generative AI > AI Application**.
    * Click "Create" (or "CREATE NEW APPLICATION").
    * Select application type: `Chat`.
    * Application name: `SocialMediaTrendAnalyzer`.
    * Region: `us-central1`.
    * Model: `Gemini` (or `Gemini Pro`).
    * Continue the agent creation process.
3.  **Connect Data Store to Agent**:
    * Once the agent is created, on its configuration page, find the "Data stores" or "Search" section.
    * Add the Data Store you just created (from `social_media_trends.daily_trends`).
4.  **Verify IAM for Agent Interaction with Data Store**:
    * Locate your Agent Assist Service Account (e.g., `service-[PROJECT_NUMBER]@gcp-sa-dialogflow.iam.gserviceaccount.com`). To get `[PROJECT_NUMBER]`, use `gcloud projects describe YOUR_GCP_PROJECT_ID --format="value(projectNumber)"`.
    * Grant the `roles/discoveryengine.user` role to this Agent Assist Service Account on your project (`YOUR_GCP_PROJECT_ID`).
    ```bash
    gcloud projects add-iam-policy-binding YOUR_GCP_PROJECT_ID \
    --member=serviceAccount:[AGENT_SERVICE_ACCOUNT_EMAIL] \
    --role=roles/discoveryengine.user
    ```
    Replace `[AGENT_SERVICE_ACCOUNT_EMAIL]` with your Agent Assist Service Account email.
5.  **Test Your AI Agent!**
    In the "Test Agent" panel on the right side of the Agent Builder console, ask questions relevant to your data (e.g., "AI trends on TikTok?", "How many views for Dance Challenge on Instagram?").
